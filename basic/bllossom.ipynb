{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\white\\anaconda3\\envs\\style\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding layer (동결)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Llama3 모델의 토크나이저를 불러옵니다.\n",
    "model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 임베딩 레이어를 정의합니다.\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.empty((128256, 4096)))  # 크기는 예시입니다.\n",
    "\n",
    "# 임베딩 레이어를 추출한 후 이를 별도의 nn.Module로 래핑합니다.\n",
    "class EmbeddingsOnlyModel(nn.Module):\n",
    "    def __init__(self, embedding_layer):\n",
    "        super(EmbeddingsOnlyModel, self).__init__()\n",
    "        self.embeddings = embedding_layer\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embeddings(input_ids)\n",
    "\n",
    "# 래핑된 모델 인스턴스를 생성합니다.\n",
    "embeddings_only_model = EmbeddingsOnlyModel(embedding_layer)\n",
    "\n",
    "# 저장된 모델 상태를 불러옵니다.\n",
    "model_path = \"embeddings_only_model.pth\"\n",
    "embeddings_only_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in embeddings_only_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer is what I need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 인코더 레이어 정의\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer_encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassificationModel(nn.Module):\n",
    "    def __init__(self, embedding_layer, embed_dim, num_heads, num_classes):\n",
    "        super(SentenceClassificationModel, self).__init__()\n",
    "        self.embeddings = embedding_layer\n",
    "        self.transformer_encoder = TransformerEncoderLayer(embed_dim, num_heads)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embeddings(input_ids)\n",
    "        x = x.transpose(0, 1)  # 트랜스포머 인코더를 위해 차원 변환\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)  # 인코더 출력의 평균을 구하여 문장 벡터로 변환\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\white\\anaconda3\\envs\\style\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "embedding_layer = nn.Embedding(128256, 4096)  # 크기는 예시입니다.\n",
    "embed_dim = 4096\n",
    "num_heads = 8\n",
    "num_classes = 500\n",
    "model = SentenceClassificationModel(embedding_layer, embed_dim, num_heads, num_classes)\n",
    "\n",
    "# 입력 텍스트를 토큰화합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MLP-KTLim/llama-3-Korean-Bllossom-8B\")\n",
    "input_text = \"안녕 얘들아!! 오랬만이야\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification output: tensor([[ 5.3601e-02, -3.3348e-03, -1.5144e-02, -5.3594e-02,  5.3084e-03,\n",
      "          4.0774e-02,  2.1909e-01,  1.2301e-01, -4.7107e-02, -2.0732e-02,\n",
      "          1.8216e-01, -4.8974e-02, -1.5258e-01, -1.5633e-04,  6.4647e-02,\n",
      "         -3.5073e-02, -1.6617e-02, -4.3771e-02, -7.9014e-04, -7.9645e-02,\n",
      "         -4.5365e-03,  1.0553e-02, -3.7300e-02, -1.0119e-01, -1.5137e-02,\n",
      "          1.8919e-02,  1.3214e-01,  3.0318e-02,  2.9091e-02,  1.2471e-01,\n",
      "         -6.4203e-02,  3.6150e-02, -2.1405e-01,  8.0541e-02,  1.4735e-01,\n",
      "         -2.3981e-03,  3.0605e-02, -7.1291e-02,  1.2022e-01,  3.5830e-02,\n",
      "          3.3254e-02, -5.4950e-02,  6.1543e-02, -1.1629e-01, -2.6629e-02,\n",
      "          2.9524e-02,  2.6034e-02, -4.8102e-02,  2.7468e-02, -6.6219e-02,\n",
      "          3.8945e-02,  1.3010e-01,  1.4149e-01,  5.3079e-02,  6.9672e-02,\n",
      "         -1.2209e-01, -3.7857e-02, -7.9546e-02,  1.5473e-01,  1.4894e-01,\n",
      "         -3.4785e-03,  2.8395e-02,  1.3767e-01,  1.8539e-02,  1.7168e-02,\n",
      "         -9.6551e-02, -1.5024e-02,  5.3846e-02,  1.4338e-01,  1.9066e-01,\n",
      "         -1.0070e-01, -4.2153e-02, -9.4615e-02,  7.8421e-02,  1.3507e-01,\n",
      "         -9.3285e-02,  1.2286e-01,  1.0661e-01, -6.6495e-02, -3.4516e-02,\n",
      "         -2.2962e-02,  5.5867e-02, -3.6212e-02,  6.6123e-02,  1.5784e-01,\n",
      "          7.9311e-04, -8.1913e-02, -4.1742e-02, -4.4567e-02,  2.0989e-02,\n",
      "          5.6065e-03, -5.1436e-02,  2.3514e-02,  7.1825e-02,  5.2702e-02,\n",
      "          3.3791e-02, -8.3225e-02, -5.5391e-02,  3.1447e-02, -3.2296e-02,\n",
      "          7.4899e-02,  1.0764e-01, -2.0371e-03, -1.1949e-02,  1.1135e-01,\n",
      "          9.3936e-02,  5.5187e-02, -3.4462e-02, -5.5998e-02, -1.0712e-01,\n",
      "          1.5970e-01, -2.0251e-01, -4.2395e-03, -3.7517e-02, -2.2738e-01,\n",
      "         -7.1264e-02,  2.3553e-02, -6.3442e-02,  2.2941e-02,  1.4685e-03,\n",
      "         -3.9175e-02, -1.2798e-01,  2.5667e-02, -1.0924e-01,  4.5298e-02,\n",
      "          9.4455e-02, -2.3287e-02,  5.1869e-02,  7.1474e-02,  2.1040e-02,\n",
      "         -5.5610e-02,  2.8431e-02,  3.0288e-02,  1.5733e-01,  1.4228e-02,\n",
      "          1.7366e-02,  1.1982e-01,  2.1608e-02,  3.0979e-02,  3.7463e-02,\n",
      "         -4.5256e-02, -2.3686e-03, -1.0129e-01,  1.1568e-01,  2.1283e-02,\n",
      "         -9.0390e-02, -1.4029e-01, -1.8367e-01, -1.2881e-01,  2.0643e-01,\n",
      "         -1.0113e-02, -2.5558e-02, -1.1995e-01, -1.4111e-01, -1.0276e-01,\n",
      "          5.4786e-02,  7.3365e-02,  2.1769e-02, -2.8886e-02, -8.6778e-02,\n",
      "          1.0704e-01,  1.7258e-02,  1.7690e-01, -4.5841e-02, -3.2017e-03,\n",
      "          1.2285e-01, -7.7212e-02, -2.2430e-02, -1.5143e-01,  3.5267e-03,\n",
      "          5.5372e-02, -4.6543e-02, -1.5426e-02, -2.2582e-01,  1.4893e-01,\n",
      "          5.8561e-02, -1.3392e-01, -3.4922e-02,  1.3094e-02, -1.0093e-01,\n",
      "         -4.6046e-02,  3.6063e-02, -7.8364e-02,  8.3634e-03,  4.2567e-02,\n",
      "         -8.8550e-02, -5.4995e-04, -5.6805e-02, -1.3423e-02,  1.0929e-01,\n",
      "         -1.0060e-01, -1.0574e-01, -8.0745e-02, -6.8126e-02,  1.7847e-02,\n",
      "          6.0776e-02,  1.6676e-01, -1.0697e-01, -1.9923e-01, -9.0010e-02,\n",
      "         -1.7172e-01,  1.8725e-02,  8.8071e-02, -5.9006e-02,  7.8288e-02,\n",
      "         -5.0559e-02,  5.8683e-02, -3.7251e-02, -4.9503e-02,  8.5653e-02,\n",
      "          5.2517e-02,  1.0208e-01, -1.2117e-02, -2.0819e-01, -5.9195e-02,\n",
      "          2.8655e-02, -4.5778e-02,  2.4998e-02, -7.8081e-02,  4.2093e-02,\n",
      "          1.8115e-01, -9.2055e-02, -1.3159e-01, -7.3870e-02,  5.2142e-02,\n",
      "         -7.1217e-02, -5.7994e-02, -1.5296e-01,  1.0158e-01,  4.0017e-02,\n",
      "         -5.6181e-02,  5.4777e-02, -1.9174e-01,  1.3845e-01, -8.5039e-02,\n",
      "          4.3544e-02, -2.0776e-02,  8.7705e-02,  4.9615e-02,  1.0872e-01,\n",
      "          2.6812e-02,  3.1701e-02,  2.2107e-01, -6.3207e-02, -5.9061e-02,\n",
      "          1.3640e-01,  1.2124e-01,  1.5303e-02,  2.6768e-02, -5.1924e-02,\n",
      "         -9.8982e-02, -7.9981e-02,  1.5459e-02, -1.3749e-01, -1.1543e-02,\n",
      "          1.0329e-01, -5.6524e-02,  1.5506e-01, -4.6354e-02,  1.4343e-01,\n",
      "          3.3148e-02, -1.0674e-01,  1.3716e-01,  1.9627e-01,  6.1029e-02,\n",
      "         -2.2184e-02,  7.2001e-02, -8.0206e-02,  1.1470e-01,  6.4875e-02,\n",
      "         -1.5605e-01, -7.2040e-02, -1.9783e-02,  7.8285e-02,  8.3262e-02,\n",
      "         -8.6978e-02, -2.6415e-02,  2.0238e-01, -4.9684e-03,  1.0901e-01,\n",
      "         -8.9681e-02,  7.2845e-02, -6.6767e-02, -2.5145e-01, -1.0614e-01,\n",
      "          1.0166e-01, -1.7839e-01, -7.5412e-02, -9.6009e-02, -3.6732e-02,\n",
      "          3.7778e-03,  5.7459e-02,  7.7033e-02,  1.0598e-01,  8.0301e-02,\n",
      "         -1.0906e-01,  4.8320e-02,  3.2745e-02,  1.2080e-01, -9.3978e-04,\n",
      "         -2.6267e-02,  4.0030e-02, -5.1384e-02,  5.8374e-02, -6.0197e-02,\n",
      "         -7.1950e-02, -3.4445e-02,  2.6476e-02,  1.3241e-01, -9.7238e-04,\n",
      "          5.5018e-02,  1.3063e-02, -3.4879e-02,  7.0564e-02, -5.8208e-02,\n",
      "          1.1402e-01,  1.3646e-01,  2.9728e-02,  2.1310e-02, -4.6995e-02,\n",
      "          1.0591e-02, -5.1776e-02,  7.6905e-02,  6.4780e-02,  2.0446e-01,\n",
      "         -7.7585e-02,  1.1492e-02, -6.2829e-02,  6.5219e-02, -1.1622e-02,\n",
      "          5.2640e-02, -5.9551e-02,  2.0519e-02, -1.0788e-01,  1.0428e-01,\n",
      "          3.1496e-02, -7.7313e-02, -2.0015e-02, -1.8591e-02, -5.8050e-02,\n",
      "         -2.8907e-02, -1.9895e-02, -6.1627e-02, -2.6115e-02, -6.2563e-02,\n",
      "         -5.7532e-02, -1.1864e-01,  5.6982e-04, -4.0062e-02,  2.3511e-01,\n",
      "         -4.9033e-02,  1.7067e-02,  2.6223e-02, -1.1036e-01,  2.0293e-02,\n",
      "         -2.2243e-01,  1.3540e-01,  8.6077e-02,  4.8386e-02, -8.3577e-02,\n",
      "          7.2834e-02,  9.7497e-03, -1.5618e-01, -1.3650e-01,  6.0850e-02,\n",
      "         -1.3590e-01, -2.3339e-02,  6.6220e-03, -7.6531e-02,  8.9434e-02,\n",
      "          5.2325e-03,  2.4522e-02, -8.1295e-02, -6.0460e-02, -1.0006e-01,\n",
      "         -2.4129e-02, -6.0858e-02,  6.6005e-02, -5.6719e-02, -6.5818e-02,\n",
      "          1.4866e-01, -3.1472e-03, -1.1686e-01, -1.2695e-01,  4.0710e-03,\n",
      "          1.5127e-01,  7.3820e-02, -1.4705e-01, -4.6108e-02,  8.6110e-02,\n",
      "          5.0444e-02,  5.2984e-02,  3.7636e-02, -5.5559e-02,  2.9892e-02,\n",
      "         -8.4550e-02,  5.5147e-02,  5.7363e-02, -4.0695e-02, -1.1408e-02,\n",
      "         -4.6143e-02,  1.0036e-01,  1.1182e-01,  1.2327e-01,  5.6047e-02,\n",
      "         -1.4752e-01, -1.3502e-01, -1.7469e-02, -1.5695e-01,  1.3517e-01,\n",
      "          2.9801e-02,  1.1869e-02,  5.1488e-02,  7.4583e-03,  7.6845e-02,\n",
      "         -2.2765e-01,  1.9503e-02,  5.9123e-02,  1.7126e-02,  1.5026e-02,\n",
      "         -1.4469e-01,  1.2416e-01,  6.3472e-02, -2.2813e-03,  6.8672e-02,\n",
      "         -5.8311e-02,  8.7559e-02, -6.5430e-02,  2.8410e-03, -5.0482e-02,\n",
      "         -7.8744e-02,  9.5817e-02,  9.6317e-02, -1.8666e-01,  8.8242e-03,\n",
      "         -4.2927e-02, -8.6669e-02, -1.2855e-01, -3.7286e-03,  3.1822e-02,\n",
      "         -3.6512e-03, -2.2938e-02,  1.8658e-02,  6.6231e-02, -1.0416e-01,\n",
      "          1.0788e-01,  2.1490e-02, -8.5267e-03,  4.8660e-02, -7.4937e-02,\n",
      "         -1.3099e-01, -6.5888e-02, -7.1738e-03, -6.3339e-02, -1.9138e-02,\n",
      "         -1.3879e-03, -3.8451e-03,  1.4385e-02, -1.1770e-01, -1.5700e-02,\n",
      "          1.8176e-02,  1.0904e-02,  2.2489e-02,  3.7646e-02, -1.0739e-01,\n",
      "          1.1356e-02, -6.4088e-03,  9.0109e-03, -1.1586e-02, -1.8002e-02,\n",
      "         -1.8984e-02,  1.5127e-01, -2.9691e-02, -7.2848e-02,  3.2404e-02,\n",
      "          5.3802e-02,  4.4423e-02,  2.5620e-02,  7.3202e-02,  2.8933e-03,\n",
      "         -1.5281e-01, -4.5006e-02,  2.5841e-02,  4.0250e-02, -4.7397e-03,\n",
      "          9.4929e-02, -3.1817e-02,  1.0002e-02,  1.0912e-01,  9.6521e-02,\n",
      "          1.2684e-02,  2.0880e-02,  3.8456e-03, -3.8112e-02,  1.0864e-01,\n",
      "          4.1952e-02, -3.1276e-02,  5.0477e-02, -7.2307e-02, -2.9988e-02]])\n"
     ]
    }
   ],
   "source": [
    "# 모델을 사용하여 예측합니다.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    print(\"Classification output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
