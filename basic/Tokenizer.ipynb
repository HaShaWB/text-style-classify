{"cells":[{"cell_type":"markdown","metadata":{"id":"d2-PqmlRLSi9"},"source":["## 토큰화"]},{"cell_type":"markdown","metadata":{},"source":["### 파일 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10465,"status":"ok","timestamp":1718953320969,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"SSj9E9DXE8-n"},"outputs":[],"source":["import pandas as pd\n","import os\n","import gc\n","from collections import Counter\n","\n","base_dir = os.getcwd() + \"/\"\n","\n","base_dir\n","\n","#base_dir = \"/content/drive/MyDrive/bookend/dev/text-style-classify/basic/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":789},"executionInfo":{"elapsed":598919,"status":"ok","timestamp":1718954007525,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"V8r8ij-DIVc5","outputId":"30388ca2-66de-441c-93ec-ffd85cf81c1a"},"outputs":[],"source":["df1 = pd.read_csv(base_dir + \"datasets/translate.csv\")\n","\"\"\"df2 = pd.read_csv(base_dir + \"datasets/written_0.csv\")\n","df3 = pd.read_csv(base_dir + \"datasets/written_1.csv\")\n","df4 = pd.read_csv(base_dir + \"datasets/written_2.csv\")\n","df5 = pd.read_csv(base_dir + \"datasets/written_3.csv\")\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### BERT 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1234,"status":"ok","timestamp":1718953406452,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"KH8TGJuiE_Am"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","def bert_tokenizer(sentence):\n","    encoding = tokenizer.encode_plus(\n","        sentence,\n","        add_special_tokens=True,\n","        max_length=64,\n","        padding=\"max_length\",\n","        return_attention_mask=False,\n","        return_tensors='pt',\n","        truncation=True\n","    )\n","    tokens = encoding['input_ids'][0]\n","    token_list = tokenizer.convert_ids_to_tokens(tokens)\n","    return pd.Series([tokens, token_list])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#df1[[\"bert_encodes\", \"bert_tokens\"]] = df1[\"sentence\"].apply(bert_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"df2[[\"bert_encodes\", \"bert_tokens\"]] = df2[\"sentence\"].apply(bert_tokenizer)\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Mecab 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from mecab import MeCab\n","\n","mecab = MeCab()\n","\n","def mecab_tokenizer(sentence):\n","    tokens = mecab.morphs(sentence)\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"df1[\"mecab_tokens\"] = df1[\"sentence\"].apply(mecab_tokenizer)\n","df1.to_pickle(base_dir + \"datasets/translate.pkl\")\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Tiktokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tiktoken\n","\n","# 사용할 인코딩 선택\n","encoding_name = \"cl100k_base\"  # \"p50k_base\", \"r50k_base\", \"gpt2\" 등으로 변경 가능\n","\n","# tiktoken 토크나이저 초기화\n","tokenizer = tiktoken.get_encoding(encoding_name)\n","\n","def tiktoken_tokenizer(sentence):\n","    # 문자열이 아닌 경우 빈 시리즈 반환\n","    if not isinstance(sentence, str):\n","        return pd.Series([[], []])\n","    \n","    # tiktoken을 사용하여 문장을 토큰화하고 ID로 변환\n","    token_ids = tokenizer.encode(sentence)\n","    # 토큰 ID를 다시 토큰으로 변환\n","    token_list = [tokenizer.decode([token_id])[0] for token_id in token_ids]\n","    return pd.Series([token_ids, token_list])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1 = pd.read_csv(base_dir + \"datasets/translate.csv\")\n","df1[[\"tiktoken_encodes\", \"tiktoken_tokens\"]] = df1[\"sentence\"].apply(tiktoken_tokenizer)\n","df1.to_pickle(base_dir + \"datasets/translate.pkl\")"]},{"cell_type":"markdown","metadata":{},"source":["### 토큰 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab_counter = Counter()\n","print(\"start\")\n","\n","# 첫 번째 데이터셋 처리\n","df = pd.read_pickle(base_dir + \"compact_datasets/translate.pkl\")\n","tokenized_sentences = df['tiktoken_encodes']\n","\n","for sentence in tokenized_sentences:\n","    for word in sentence:\n","        vocab_counter[word] += 1\n","\n","del tokenized_sentences\n","gc.collect()\n","\n","# 나머지 데이터셋 처리\n","for i in range(20):\n","    print(f\"file : {i}\")\n","    df = pd.read_pickle(base_dir + f\"compact_datasets/written_{i}.pkl\")\n","    tokenized_sentences = df['tiktoken_encodes']\n","    \n","    for sentence in tokenized_sentences:\n","        for word in sentence:\n","            if type(word) == int:\n","                vocab_counter[word] += 1\n","            else:\n","                print(f\"error {word}\")\n","\n","    del tokenized_sentences\n","    gc.collect()\n","\n","\n","print(f\"Total unique words: {len(vocab_counter)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 상위 3만 개 단어 추출\n","top_30000_words = vocab_counter.most_common(30000)\n","\n","word_to_index = {word: index+2 for index, (word, _) in enumerate(top_30000_words)}\n","word_to_index['PAD'] = 0\n","word_to_index['OOV'] = 1\n","\n","# dictionary 출력 확인\n","print(f\"Top 30000 words mapping: {word_to_index}\")\n","\n","# dictionary를 파일로 저장 (옵션)\n","import pickle\n","with open(base_dir + \"compact_datasets/vocab_ver1.pkl\", \"wb\") as f:\n","    pickle.dump(word_to_index, f)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# 데이터프레임에 적용\n","df = pd.read_pickle(base_dir + \"compact_datasets/translate.pkl\")\n","df['encoded_sentence'] = df['tiktoken_encodes'].apply(lambda x: [word_to_index.get(word, 1) for word in x])\n","df.to_pickle(base_dir + \"compact_datasets/translate.pkl\")\n","\n","del df\n","gc.collect()\n","\n","for i in range(20):\n","    print(i)\n","    df = pd.read_pickle(base_dir + f\"compact_datasets/written_{i}.pkl\")\n","    df['encoded_sentence'] = df['tiktoken_encodes'].apply(lambda x: [word_to_index.get(word, 1) for word in x])\n","    df.to_pickle(base_dir + f\"compact_datasets/written_{i}.pkl\")\n","    del df\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"9GHJXApqJq9b"},"source":["## author 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1 = pd.read_pickle(base_dir + \"datasets/translate.pkl\")\n","df2 = pd.read_pickle(base_dir + \"datasets/written_0.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["author_set = set(df1['author'].to_list())\n","\n","df1 = None\n","gc.collect()\n","\n","\n","for i in range(20):\n","    print(i)\n","    gc.collect()\n","    df = pd.read_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\")\n","    temp_set = set(df['author'].to_list())\n","    author_set = author_set.union(temp_set)\n","    df = None\n","\n","\n","author_dict = {author : i for i, author in enumerate(author_set)}\n","\n","len(author_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1718955900921,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"-gebcUFmJybX"},"outputs":[],"source":["import json\n","\n","with open(base_dir + \"datasets/author_encoding.json\", \"w\") as json_file:\n","    json.dump(author_dict, json_file, indent=4)\n","\n","df1 = pd.read_pickle(base_dir + \"datasets/translate.pkl\")\n","df1['encoded_author'] = df1['author'].apply(lambda x : author_dict[x])\n","df1.to_pickle(base_dir + \"datasets/translate.pkl\")\n","\n","df1 = None\n","gc.collect()\n","\n","\n","for i in range(20):\n","    df = pd.read_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\")\n","\n","    df['encoded_author'] = df['author'].apply(lambda x : author_dict[x])\n","\n","    df.to_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\") \n","\n","    df = None\n","\n","    gc.collect()\n","\n","\n","len(author_set)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_pickle(base_dir + \"datasets/translate.pkl\")\n","df.to_csv(base_dir + \"datasets/translate.csv\", index=False, encoding='utf-8-sig')\n","df = None\n","gc.collect()\n","\n","for i in range(20):\n","    print(i)\n","    df = pd.read_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\")\n","    df.to_csv(base_dir + \"datasets/written_\" + str(i) + \".csv\", index=False, encoding='utf-8-sig')\n","    df = None\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Compactize"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_pickle(base_dir + \"datasets/translate.pkl\")\n","df = df[['tiktoken_encodes', 'encoded_author']]\n","df.to_pickle(base_dir + \"compact_datasets/translate.pkl\")\n","\n","del df\n","gc.callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(20):\n","    print(i)\n","    df = pd.read_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\")\n","    df = df[['tiktoken_encodes', 'encoded_author']]\n","    df.to_pickle(base_dir + \"compact_datasets/written_\" + str(i) + \".pkl\")\n","    df = None\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"0j_276uZLaCD"},"source":["### 장르 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgjG9F27LbIn"},"outputs":[],"source":["#df = pd.read_pickle(base_dir +  f'data/all.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":541,"status":"ok","timestamp":1718955950920,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"uHNv_iOhLdgI"},"outputs":[],"source":["df['class'] = df['Classify_1']+\"/\" + df['Classify_2']\n","\n","classify_set = set(df['class'].to_list())\n","\n","classify_dict = {classs : i for i, classs in enumerate(classify_set)}\n","\n","import json\n","\n","with open(base_dir + \"data/class_encoding.json\", \"w\") as json_file:\n","    json.dump(classify_dict, json_file, indent=4)\n","\n","df['encoded_class'] = df['class'].apply(lambda x : classify_dict[x])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.to_pickle(base_dir +  f'data/labeled_compact.pkl')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
