{"cells":[{"cell_type":"markdown","metadata":{"id":"d2-PqmlRLSi9"},"source":["### 토큰화"]},{"cell_type":"markdown","metadata":{},"source":["### 파일 로드"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10465,"status":"ok","timestamp":1718953320969,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"SSj9E9DXE8-n"},"outputs":[{"data":{"text/plain":["'c:\\\\Dev\\\\Github\\\\text-style-classify\\\\basic/'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import os\n","import gc\n","\n","base_dir = os.getcwd() + \"/\"\n","\n","base_dir\n","\n","#base_dir = \"/content/drive/MyDrive/bookend/dev/text-style-classify/basic/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21155,"status":"ok","timestamp":1718953398280,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"TIeuSfywmu7k","outputId":"cc3be74d-e1c2-463c-a805-a51af5699db7"},"outputs":[],"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":789},"executionInfo":{"elapsed":598919,"status":"ok","timestamp":1718954007525,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"V8r8ij-DIVc5","outputId":"30388ca2-66de-441c-93ec-ffd85cf81c1a"},"outputs":[],"source":["df1 = pd.read_csv(base_dir + \"datasets/translate.csv\")\n","\"\"\"df2 = pd.read_csv(base_dir + \"datasets/written_0.csv\")\n","df3 = pd.read_csv(base_dir + \"datasets/written_1.csv\")\n","df4 = pd.read_csv(base_dir + \"datasets/written_2.csv\")\n","df5 = pd.read_csv(base_dir + \"datasets/written_3.csv\")\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### BERT 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1234,"status":"ok","timestamp":1718953406452,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"KH8TGJuiE_Am"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","def bert_tokenizer(sentence):\n","    encoding = tokenizer.encode_plus(\n","        sentence,\n","        add_special_tokens=True,\n","        max_length=64,\n","        padding=\"max_length\",\n","        return_attention_mask=False,\n","        return_tensors='pt',\n","        truncation=True\n","    )\n","    tokens = encoding['input_ids'][0]\n","    token_list = tokenizer.convert_ids_to_tokens(tokens)\n","    return pd.Series([tokens, token_list])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#df1[[\"bert_encodes\", \"bert_tokens\"]] = df1[\"sentence\"].apply(bert_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"df2[[\"bert_encodes\", \"bert_tokens\"]] = df2[\"sentence\"].apply(bert_tokenizer)\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Mecab 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from mecab import MeCab\n","\n","mecab = MeCab()\n","\n","def mecab_tokenizer(sentence):\n","    tokens = mecab.morphs(sentence)\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"df1[\"mecab_tokens\"] = df1[\"sentence\"].apply(mecab_tokenizer)\n","df1.to_pickle(base_dir + \"datasets/translate.pkl\")\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Tiktokenizer"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import tiktoken\n","\n","# 사용할 인코딩 선택\n","encoding_name = \"cl100k_base\"  # \"p50k_base\", \"r50k_base\", \"gpt2\" 등으로 변경 가능\n","\n","# tiktoken 토크나이저 초기화\n","tokenizer = tiktoken.get_encoding(encoding_name)\n","\n","def tiktoken_tokenizer(sentence):\n","    # 문자열이 아닌 경우 빈 시리즈 반환\n","    if not isinstance(sentence, str):\n","        return pd.Series([[], []])\n","    \n","    # tiktoken을 사용하여 문장을 토큰화하고 ID로 변환\n","    token_ids = tokenizer.encode(sentence)\n","    # 토큰 ID를 다시 토큰으로 변환\n","    token_list = [tokenizer.decode([token_id])[0] for token_id in token_ids]\n","    return pd.Series([token_ids, token_list])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df1 = pd.read_csv(base_dir + \"datasets/translate.csv\")\n","df1[[\"tiktoken_encodes\", \"tiktoken_tokens\"]] = df1[\"sentence\"].apply(tiktoken_tokenizer)\n","df1.to_pickle(base_dir + \"datasets/translate.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"9GHJXApqJq9b"},"source":["### author 인코딩"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df1 = pd.read_pickle(base_dir + \"datasets/translate.pkl\")\n","df2 = pd.read_pickle(base_dir + \"datasets/written_0.pkl\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n"]},{"data":{"text/plain":["10373"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["author_set = set(df1['author'].to_list())\n","\n","df1 = None\n","gc.collect()\n","\n","\n","for i in range(20):\n","    print(i)\n","    gc.collect()\n","    df = pd.read_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\")\n","    temp_set = set(df['author'].to_list())\n","    author_set = author_set.union(temp_set)\n","    df = None\n","\n","\n","author_dict = {author : i for i, author in enumerate(author_set)}\n","\n","len(author_dict)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1718955900921,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"-gebcUFmJybX"},"outputs":[{"data":{"text/plain":["10373"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import json\n","\n","with open(base_dir + \"datasets/author_encoding.json\", \"w\") as json_file:\n","    json.dump(author_dict, json_file, indent=4)\n","\n","df1 = pd.read_pickle(base_dir + \"datasets/translate.pkl\")\n","df1['encoded_author'] = df1['author'].apply(lambda x : author_dict[x])\n","df1.to_pickle(base_dir + \"datasets/translate.pkl\")\n","\n","df1 = None\n","gc.collect()\n","\n","\n","for i in range(20):\n","    df = pd.read_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\")\n","\n","    df['encoded_author'] = df['author'].apply(lambda x : author_dict[x])\n","\n","    df.to_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\") \n","\n","    df = None\n","\n","    gc.collect()\n","\n","\n","len(author_set)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["df = pd.read_pickle(base_dir + \"datasets/translate.pkl\")\n","df.to_csv(base_dir + \"datasets/translate.csv\", index=False, encoding='utf-8-sig')\n","df = None\n","gc.collect()\n","\n","for i in range(20):\n","    print(i)\n","    df = pd.read_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\")\n","    df.to_csv(base_dir + \"datasets/written_\" + str(i) + \".csv\", index=False, encoding='utf-8-sig')\n","    df = None\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"0j_276uZLaCD"},"source":["### 장르 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgjG9F27LbIn"},"outputs":[],"source":["#df = pd.read_pickle(base_dir +  f'data/all.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":541,"status":"ok","timestamp":1718955950920,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"uHNv_iOhLdgI"},"outputs":[],"source":["df['class'] = df['Classify_1']+\"/\" + df['Classify_2']\n","\n","classify_set = set(df['class'].to_list())\n","\n","classify_dict = {classs : i for i, classs in enumerate(classify_set)}\n","\n","import json\n","\n","with open(base_dir + \"data/class_encoding.json\", \"w\") as json_file:\n","    json.dump(classify_dict, json_file, indent=4)\n","\n","df['encoded_class'] = df['class'].apply(lambda x : classify_dict[x])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.to_pickle(base_dir +  f'data/labeled_compact.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
