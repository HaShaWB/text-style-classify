{"cells":[{"cell_type":"markdown","metadata":{"id":"d2-PqmlRLSi9"},"source":["### 토큰화"]},{"cell_type":"markdown","metadata":{},"source":["### 파일 로드"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10465,"status":"ok","timestamp":1718953320969,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"SSj9E9DXE8-n"},"outputs":[{"data":{"text/plain":["'c:\\\\Dev\\\\Github\\\\text-style-classify\\\\basic/'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import os\n","\n","base_dir = os.getcwd() + \"/\"\n","\n","base_dir\n","\n","#base_dir = \"/content/drive/MyDrive/bookend/dev/text-style-classify/basic/\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def split_dataframe(df, n):\n","    split_dfs = []\n","    chunk_size = len(df) // n\n","    for i in range(n):\n","        start_index = i * chunk_size\n","        if i == n - 1:  # 마지막 부분은 나머지를 포함\n","            end_index = len(df)\n","        else:\n","            end_index = (i + 1) * chunk_size\n","        split_dfs.append(df.iloc[start_index:end_index].copy())\n","    return split_dfs\n","\n","dfs = split_dataframe(pd.read_csv(base_dir + \"datasets/written.csv\"), 10)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["for i in range(len(dfs)):\n","    dfs[i].to_csv(base_dir + \"datasets/written_\" + str(i) + \".csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21155,"status":"ok","timestamp":1718953398280,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"TIeuSfywmu7k","outputId":"cc3be74d-e1c2-463c-a805-a51af5699db7"},"outputs":[],"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":789},"executionInfo":{"elapsed":598919,"status":"ok","timestamp":1718954007525,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"V8r8ij-DIVc5","outputId":"30388ca2-66de-441c-93ec-ffd85cf81c1a"},"outputs":[],"source":["\"\"\"df1 = pd.read_csv(base_dir + \"datasets/translate.csv\")\n","df2 = pd.read_csv(base_dir + \"datasets/written_0.csv\")\n","df3 = pd.read_csv(base_dir + \"datasets/written_1.csv\")\n","df4 = pd.read_csv(base_dir + \"datasets/written_2.csv\")\n","df5 = pd.read_csv(base_dir + \"datasets/written_3.csv\")\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df2 = pd.read_csv(base_dir + \"datasets/written.csv\")\n","\n","df2s = []\n","\n","for i in range(10):\n","    print(f\"Reading {i}\")\n","    df2s.append(pd.read_pickle(base_dir + \"datasets/written_\" + str(i) + \".pkl\"))\n","df2 = pd.concat(df2s)\n","\n","df2 = df2[['author', 'sentence', 'tiktoken_encodes']]\n","\n","df2.to_pickle(base_dir + \"datasets/written_t1.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df3s = []\n","\n","for i in range(3):\n","    print(f\"Reading {i+3}\")\n","    df3s.append(pd.read_pickle(base_dir + \"datasets/written_\" + str(i+3) + \".pkl\"))\n","df3 = pd.concat(df3s)\n","\n","df3 = df3[['author', 'sentence', 'tiktoken_encodes']]\n","\n","df3.to_pickle(base_dir + \"datasets/written_t2.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df4s = []\n","\n","for i in range(4):\n","    print(f\"Reading {i+6}\")\n","    df4s.append(pd.read_pickle(base_dir + \"datasets/written_\" + str(i+6) + \".pkl\"))\n","df4 = pd.concat(df4s)\n","\n","df4 = df4[['author', 'sentence', 'tiktoken_encodes']]\n","\n","df4.to_pickle(base_dir + \"datasets/written_t3.pkl\")"]},{"cell_type":"markdown","metadata":{},"source":["### BERT 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1234,"status":"ok","timestamp":1718953406452,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"KH8TGJuiE_Am"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","def bert_tokenizer(sentence):\n","    encoding = tokenizer.encode_plus(\n","        sentence,\n","        add_special_tokens=True,\n","        max_length=64,\n","        padding=\"max_length\",\n","        return_attention_mask=False,\n","        return_tensors='pt',\n","        truncation=True\n","    )\n","    tokens = encoding['input_ids'][0]\n","    token_list = tokenizer.convert_ids_to_tokens(tokens)\n","    return pd.Series([tokens, token_list])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#df1[[\"bert_encodes\", \"bert_tokens\"]] = df1[\"sentence\"].apply(bert_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"df2[[\"bert_encodes\", \"bert_tokens\"]] = df2[\"sentence\"].apply(bert_tokenizer)\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Mecab 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from mecab import MeCab\n","\n","mecab = MeCab()\n","\n","def mecab_tokenizer(sentence):\n","    tokens = mecab.morphs(sentence)\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"df1[\"mecab_tokens\"] = df1[\"sentence\"].apply(mecab_tokenizer)\n","df1.to_pickle(base_dir + \"datasets/translate.pkl\")\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Tiktokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tiktoken\n","\n","# 사용할 인코딩 선택\n","encoding_name = \"cl100k_base\"  # \"p50k_base\", \"r50k_base\", \"gpt2\" 등으로 변경 가능\n","\n","# tiktoken 토크나이저 초기화\n","tokenizer = tiktoken.get_encoding(encoding_name)\n","\n","def tiktoken_tokenizer(sentence):\n","    # 문자열이 아닌 경우 빈 시리즈 반환\n","    if not isinstance(sentence, str):\n","        return pd.Series([[], []])\n","    \n","    # tiktoken을 사용하여 문장을 토큰화하고 ID로 변환\n","    token_ids = tokenizer.encode(sentence)\n","    # 토큰 ID를 다시 토큰으로 변환\n","    token_list = [tokenizer.decode([token_id])[0] for token_id in token_ids]\n","    return pd.Series([token_ids, token_list])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1[[\"tiktoken_encodes\", \"tiktoken_tokens\"]] = df1[\"sentence\"].apply(tiktoken_tokenizer)\n","#df2[[\"tiktoken_encodes\", \"tiktoken_tokens\"]] = df2[\"sentence\"].apply(tiktoken_tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["### 파일 합쳐서 저장"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.concat([df1, df2], ignore_index=True)\n","df.to_pickle(base_dir + \"datasets/total.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"9GHJXApqJq9b"},"source":["### author 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1718955898746,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"2pUpvgDeJs23"},"outputs":[],"source":["#df = pd.read_pickle(base_dir +  f'data/all.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1718955900921,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"-gebcUFmJybX"},"outputs":[],"source":["author_set = set(df['Author'].to_list())\n","\n","author_dict = {author : i for i, author in enumerate(author_set)}\n","\n","import json\n","\n","with open(base_dir + \"data/author_encoding.json\", \"w\") as json_file:\n","    json.dump(author_dict, json_file, indent=4)\n","\n","df['encoded_author'] = df['Author'].apply(lambda x : author_dict[x])"]},{"cell_type":"markdown","metadata":{"id":"0j_276uZLaCD"},"source":["### 장르 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgjG9F27LbIn"},"outputs":[],"source":["#df = pd.read_pickle(base_dir +  f'data/all.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":541,"status":"ok","timestamp":1718955950920,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"uHNv_iOhLdgI"},"outputs":[],"source":["df['class'] = df['Classify_1']+\"/\" + df['Classify_2']\n","\n","classify_set = set(df['class'].to_list())\n","\n","classify_dict = {classs : i for i, classs in enumerate(classify_set)}\n","\n","import json\n","\n","with open(base_dir + \"data/class_encoding.json\", \"w\") as json_file:\n","    json.dump(classify_dict, json_file, indent=4)\n","\n","df['encoded_class'] = df['class'].apply(lambda x : classify_dict[x])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.to_pickle(base_dir +  f'data/labeled_compact.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
