{"cells":[{"cell_type":"markdown","metadata":{},"source":["### 파일 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10465,"status":"ok","timestamp":1718953320969,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"SSj9E9DXE8-n"},"outputs":[],"source":["import pandas as pd\n","import os\n","import gc\n","from collections import Counter\n","\n","base_dir = os.getcwd() + \"/\"\n","\n","FILE_SPLITS = 20"]},{"cell_type":"markdown","metadata":{},"source":["### 파일 분할"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_dataframe(df, n):\n","    split_dfs = []\n","    chunk_size = len(df) // n\n","    for i in range(n):\n","        start_index = i * chunk_size\n","        if i == n - 1:  # 마지막 부분은 나머지를 포함\n","            end_index = len(df)\n","        else:\n","            end_index = (i + 1) * chunk_size\n","        split_dfs.append(df.iloc[start_index:end_index].copy())\n","    return split_dfs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(\"datasets/written.csv\")\n","\n","df = df.sample(frac=1).reset_index(drop=True)\n","\n","df.to_pickle(\"datasets/written/mixed_written.pkl\")\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dfs = split_dataframe(df, FILE_SPLITS)\n","\n","del df\n","gc.collect()\n","\n","for i, df in enumerate(dfs):\n","    print(f\"Writing : {i}\")\n","    df.to_csv(f\"datasets/written/written_{i}.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"d2-PqmlRLSi9"},"source":["## 토큰화"]},{"cell_type":"markdown","metadata":{},"source":["### BERT 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1234,"status":"ok","timestamp":1718953406452,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"KH8TGJuiE_Am"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","def bert_tokenizer(sentence):\n","    encoding = tokenizer.encode_plus(\n","        sentence,\n","        add_special_tokens=True,\n","        max_length=64,\n","        padding=\"max_length\",\n","        return_attention_mask=False,\n","        return_tensors='pt',\n","        truncation=True\n","    )\n","    tokens = encoding['input_ids'][0]\n","    token_list = tokenizer.convert_ids_to_tokens(tokens)\n","    return pd.Series([tokens, token_list])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#df1[[\"bert_encodes\", \"bert_tokens\"]] = df1[\"sentence\"].apply(bert_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"df2[[\"bert_encodes\", \"bert_tokens\"]] = df2[\"sentence\"].apply(bert_tokenizer)\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Mecab 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from mecab import MeCab\n","\n","mecab = MeCab()\n","\n","def mecab_tokenizer(sentence):\n","    tokens = mecab.morphs(sentence)\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"df1[\"mecab_tokens\"] = df1[\"sentence\"].apply(mecab_tokenizer)\n","df1.to_pickle(base_dir + \"datasets/translate.pkl\")\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Tiktokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tiktoken\n","\n","# 사용할 인코딩 선택\n","encoding_name = \"cl100k_base\"  # \"p50k_base\", \"r50k_base\", \"gpt2\" 등으로 변경 가능\n","\n","# tiktoken 토크나이저 초기화\n","tokenizer = tiktoken.get_encoding(encoding_name)\n","\n","def tiktoken_tokenizer(sentence):\n","    # 문자열이 아닌 경우 빈 시리즈 반환\n","    if not isinstance(sentence, str):\n","        return []\n","    \n","    # tiktoken을 사용하여 문장을 토큰화하고 ID로 변환\n","    token_ids = tokenizer.encode(sentence)\n","\n","    return token_ids"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(FILE_SPLITS):\n","    print(f\"tackling {i}\")\n","    df = pd.read_csv(f\"datasets/written/written_{i}.csv\")\n","    df[\"tiktoken_tokens\"] = df[\"sentence\"].apply(tiktoken_tokenizer)\n","    df.to_pickle(f\"datasets/written/written_{i}.pkl\")\n","\n","    del df\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["### 토큰 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab_counter = Counter()\n","print(\"start\")\n","\n","\n","# 나머지 데이터셋 처리\n","for i in range(FILE_SPLITS):\n","    print(f\"file : {i}\")\n","    df = pd.read_pickle(base_dir + f\"datasets/written/written_{i}.pkl\")\n","    tokenized_sentences = df['tiktoken_tokens']\n","    \n","    for sentence in tokenized_sentences:\n","        for word in sentence:\n","            if type(word) == int:\n","                vocab_counter[word] += 1\n","            else:\n","                print(f\"error {word}\")\n","\n","    del tokenized_sentences\n","    gc.collect()\n","\n","\n","print(f\"Total unique words: {len(vocab_counter)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 상위 3만 개 단어 추출\n","top_30000_words = vocab_counter.most_common(30000)\n","\n","word_to_index = {word: index+2 for index, (word, _) in enumerate(top_30000_words)}\n","word_to_index['PAD'] = 0\n","word_to_index['OOV'] = 1\n","\n","# dictionary 출력 확인\n","print(f\"Top 30000 words mapping: {word_to_index}\")\n","\n","# dictionary를 파일로 저장 (옵션)\n","import pickle\n","with open(base_dir + \"datasets/written/vocab_ver1.pkl\", \"wb\") as f:\n","    pickle.dump(word_to_index, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(FILE_SPLITS):\n","    print(i)\n","    df = pd.read_pickle(base_dir + f\"datasets/written/written_{i}.pkl\")\n","    df['encoded_sentence'] = df['tiktoken_tokens'].apply(lambda x: [word_to_index.get(word, 1) if isinstance(word, int) else '' for word in x])\n","    df.to_pickle(base_dir + f\"datasets/written/written_{i}.pkl\")\n","    del df\n","    gc.collect()\n"]},{"cell_type":"markdown","metadata":{"id":"9GHJXApqJq9b"},"source":["## author 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["author_set = set()\n","\n","\n","for i in range(FILE_SPLITS):\n","    print(i)\n","    gc.collect()\n","    df = pd.read_pickle(f\"datasets/written/written_{i}.pkl\")\n","    temp_set = set(df['author'].to_list())\n","    author_set = author_set.union(temp_set)\n","    df = None\n","\n","\n","author_dict = {author : i for i, author in enumerate(author_set)}\n","\n","len(author_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1718955900921,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"-gebcUFmJybX"},"outputs":[],"source":["import json\n","\n","with open(base_dir + \"datasets/author_encoding.json\", \"w\") as json_file:\n","    json.dump(author_dict, json_file, indent=4)\n","\n","\n","for i in range(FILE_SPLITS):\n","    print(i)\n","    df = pd.read_pickle(f\"datasets/written/written_{i}.pkl\")\n","\n","    df['encoded_author'] = df['author'].apply(lambda x : author_dict[x])\n","\n","    df.to_pickle(f\"datasets/written/written_{i}.pkl\") \n","\n","    df = None\n","\n","    gc.collect()\n","\n","\n","len(author_set)"]},{"cell_type":"markdown","metadata":{},"source":["## Compactize"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(FILE_SPLITS):\n","    print(i)\n","    df = pd.read_pickle(f\"datasets/written/written_{i}.pkl\")\n","    df = df[['encoded_sentence', 'encoded_author']]\n","    df.to_pickle(f\"datasets/written/written_{i}.pkl\" )\n","    df = None\n","    gc.collect()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n"]}],"source":["df = pd.DataFrame()\n","\n","for i in range(FILE_SPLITS):\n","    print(i)\n","    temp = pd.read_pickle(f\"datasets/written/written_{i}.pkl\")\n","    df = pd.concat([df, temp], ignore_index=True)\n","    temp = None\n","    gc.collect()\n","\n","df.to_pickle(\"datasets/written_total.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"0j_276uZLaCD"},"source":["## 장르 인코딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgjG9F27LbIn"},"outputs":[],"source":["#df = pd.read_pickle(base_dir +  f'data/all.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":541,"status":"ok","timestamp":1718955950920,"user":{"displayName":"white blue","userId":"17600182665673203811"},"user_tz":-540},"id":"uHNv_iOhLdgI"},"outputs":[],"source":["df['class'] = df['Classify_1']+\"/\" + df['Classify_2']\n","\n","classify_set = set(df['class'].to_list())\n","\n","classify_dict = {classs : i for i, classs in enumerate(classify_set)}\n","\n","import json\n","\n","with open(base_dir + \"data/class_encoding.json\", \"w\") as json_file:\n","    json.dump(classify_dict, json_file, indent=4)\n","\n","df['encoded_class'] = df['class'].apply(lambda x : classify_dict[x])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.to_pickle(base_dir +  f'data/labeled_compact.pkl')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
